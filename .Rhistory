# Calculate total variation distance (for X and Y)
tvd_X <- sum(abs(X_prime - X))  # Placeholder for the actual TV distance computation
tvd_Y <- sum(abs(Y_prime - Y))  # Placeholder for the actual TV distance computation
# Compute the difference in regression coefficients
diff_beta0 <- (beta0_orig - mean(Y_prime - X_prime))^2
diff_beta1 <- (beta1_orig - mean(Y_prime / X_prime))^2
# Compute R^2 differences for each category in Z
R2_diff <- sum((R2_orig - p)^2)  # Placeholder for actual R^2 differences calculation
# Inter-cluster distance based on Z (as a placeholder)
inter_cluster_dist <- sum(abs(table(Z) - mean(Z)))  # Placeholder for actual inter-cluster calculation
# Combine all terms into the final loss function
loss <- lambda1 * (tvd_X + tvd_Y) + lambda2 * (diff_beta0 + diff_beta1) +
lambda3 * R2_diff + lambda4 * inter_cluster_dist
return(loss)
}
res = get_simpsons_paradox_c(x, y, z, t, sd_x = 0.07, sd_y = 0.07, lambda4 = 5)
#' Simulated Annealing Optimization with Categorical Variable and R^2 Differences
#'
#' This function implements the Simulated Annealing algorithm to optimize a solution
#' based on the total variation distance, changes in regression coefficients,
#' R-squared differences, and inter-cluster distance, with respect to a set of
#' categorical and continuous variables.
#'
#' @param X A numeric vector or matrix of input data (independent variable).
#' @param Y A numeric vector of the dependent variable (target).
#' @param Z A categorical variable (vector), used for grouping data in the analysis.
#' @param X_st A numeric vector of starting values for the composition method of X.
#' @param Y_st A numeric vector of starting values for the composition method of Y.
#' @param p A numeric vector representing the target R^2 values for each category in Z.
#' @param sd_x Standard deviation for the noise added to X during the perturbation (default is 0.05).
#' @param sd_y Standard deviation for the noise added to Y during the perturbation (default is 0.05).
#' @param lambda1 Regularization parameter for the total variation distance term (default is 1).
#' @param lambda2 Regularization parameter for the coefficient difference term (default is 1).
#' @param lambda3 Regularization parameter for the R^2 difference term (default is 1).
#' @param lambda4 Regularization parameter for the inter-cluster distance term (default is 1).
#' @param max_iter Maximum number of iterations for the annealing process (default is 1000).
#' @param initial_temp Initial temperature for the annealing process (default is 1.0).
#' @param cooling_rate The rate at which the temperature cools down during annealing (default is 0.99).
#'
#' @return A list with the optimized values of X_prime and Y_prime.
#' @export
simulated_annealing_SL <- function(X, Y, Z, X_st, Y_st, p, sd_x = 0.05, sd_y = 0.05,
lambda1 = 1, lambda2 = 1, lambda3 = 1, lambda4 = 1,
max_iter = 1000, initial_temp = 1.0, cooling_rate = 0.99) {
# Initialize X_prime and Y_prime from the starting composition method output
X_prime <- X_st
Y_prime <- Y_st
best_X_prime <- X_prime
best_Y_prime <- Y_prime
# Compute original regression coefficients (beta0 and beta1) using the full dataset
fit_orig <- lm(Y ~ X)
beta0_orig <- coef(fit_orig)[1]
beta1_orig <- coef(fit_orig)[2]
# Compute R-squared for each category in the categorical variable Z
categories <- unique(Z)
R2_orig <- sapply(categories, function(cat) {
fit_cat <- lm(Y[Z == cat] ~ X[Z == cat])
summary(fit_cat)$r.squared
})
# Initialize best_loss using the objective function with initial X_prime and Y_prime
best_loss <- objective_function_SL(X_prime, Y_prime, X, Y, Z, p, beta0_orig, beta1_orig,
lambda1, lambda2, lambda3, lambda4, R2_orig)
print(best_loss)
# Start the simulated annealing process with the initial temperature
temp <- initial_temp
for (i in 1:max_iter) {
# Generate new solutions by adding random noise to X_prime and Y_prime
new_X_prime <- X_prime + rnorm(length(X_prime), 0, sd_x)
new_Y_prime <- Y_prime + rnorm(length(Y_prime), 0, sd_y)
# Compute the new loss using the updated values
new_loss <- objective_function_SL(new_X_prime, new_Y_prime, X, Y, Z, p, beta0_orig,
beta1_orig, lambda1, lambda2, lambda3, lambda4, R2_orig)
# Accept the new solution based on the Metropolis criterion
if (new_loss < best_loss || runif(1) < exp((best_loss - new_loss) / temp)) {
X_prime <- new_X_prime
Y_prime <- new_Y_prime
best_loss <- new_loss
best_X_prime <- X_prime
best_Y_prime <- Y_prime
}
# Decrease the temperature according to the cooling rate
temp <- temp * cooling_rate
# Check if the temperature is low enough to stop
if (temp < 1e-6) {
break
}
}
# Return the best solution found
return(list(X_prime = best_X_prime, Y_prime = best_Y_prime))
}
#' Simulated Annealing Optimization with Categorical Variable and R^2 Differences
#'
#' This function implements the Simulated Annealing algorithm to optimize a solution
#' based on the total variation distance, changes in regression coefficients,
#' R-squared differences, and inter-cluster distance, with respect to a set of
#' categorical and continuous variables.
#'
#' @param X A numeric vector or matrix of input data (independent variable).
#' @param Y A numeric vector of the dependent variable (target).
#' @param Z A categorical variable (vector), used for grouping data in the analysis.
#' @param X_st A numeric vector of starting values for the composition method of X.
#' @param Y_st A numeric vector of starting values for the composition method of Y.
#' @param p A numeric vector representing the target R^2 values for each category in Z.
#' @param sd_x Standard deviation for the noise added to X during the perturbation (default is 0.05).
#' @param sd_y Standard deviation for the noise added to Y during the perturbation (default is 0.05).
#' @param lambda1 Regularization parameter for the total variation distance term (default is 1).
#' @param lambda2 Regularization parameter for the coefficient difference term (default is 1).
#' @param lambda3 Regularization parameter for the R^2 difference term (default is 1).
#' @param lambda4 Regularization parameter for the inter-cluster distance term (default is 1).
#' @param max_iter Maximum number of iterations for the annealing process (default is 1000).
#' @param initial_temp Initial temperature for the annealing process (default is 1.0).
#' @param cooling_rate The rate at which the temperature cools down during annealing (default is 0.99).
#'
#' @return A list with the optimized values of X_prime and Y_prime.
#' @export
simulated_annealing_SL <- function(X, Y, Z, X_st, Y_st, p, sd_x = 0.05, sd_y = 0.05,
lambda1 = 1, lambda2 = 1, lambda3 = 1, lambda4 = 1,
max_iter = 1000, initial_temp = 1.0, cooling_rate = 0.99) {
# Initialize X_prime and Y_prime from the starting composition method output
X_prime <- X_st
Y_prime <- Y_st
best_X_prime <- X_prime
best_Y_prime <- Y_prime
# Compute original regression coefficients (beta0 and beta1) using the full dataset
fit_orig <- lm(Y ~ X)
beta0_orig <- coef(fit_orig)[1]
beta1_orig <- coef(fit_orig)[2]
# Compute R-squared for each category in the categorical variable Z
categories <- unique(Z)
R2_orig <- sapply(categories, function(cat) {
fit_cat <- lm(Y[Z == cat] ~ X[Z == cat])
summary(fit_cat)$r.squared
})
# Initialize best_loss using the objective function with initial X_prime and Y_prime
best_loss <- objective_function_SL(X_prime, Y_prime, X, Y, Z, p, beta0_orig, beta1_orig,
lambda1, lambda2, lambda3, lambda4, R2_orig)
print(best_loss)
# Start the simulated annealing process with the initial temperature
temp <- initial_temp
for (i in 1:max_iter) {
# Generate new solutions by adding random noise to X_prime and Y_prime
new_X_prime <- X_prime + rnorm(length(X_prime), 0, sd_x)
new_Y_prime <- Y_prime + rnorm(length(Y_prime), 0, sd_y)
# Compute the new loss using the updated values
new_loss <- objective_function_SL(new_X_prime, new_Y_prime, X, Y, Z, p, beta0_orig,
beta1_orig, lambda1, lambda2, lambda3, lambda4, R2_orig)
# Accept the new solution based on the Metropolis criterion
if (new_loss < best_loss || runif(1) < exp((best_loss - new_loss) / temp)) {
X_prime <- new_X_prime
Y_prime <- new_Y_prime
best_loss <- new_loss
best_X_prime <- X_prime
best_Y_prime <- Y_prime
}
# Decrease the temperature according to the cooling rate
temp <- temp * cooling_rate
# Check if the temperature is low enough to stop
if (temp < 1e-6) {
break
}
}
# Return the best solution found
return(list(X_prime = best_X_prime, Y_prime = best_Y_prime))
}
# Internal helper function for the objective function calculation.
# This function calculates the loss for the current solution using total variation
# distance, coefficient differences, R^2 differences, and inter-cluster distance.
#' @noRd
objective_function_SL <- function(X_prime, Y_prime, X, Y, Z, p, beta0_orig, beta1_orig,
lambda1, lambda2, lambda3, lambda4, R2_orig) {
# Calculate total variation distance (for X and Y)
tvd_X <- sum(abs(X_prime - X))  # Placeholder for the actual TV distance computation
tvd_Y <- sum(abs(Y_prime - Y))  # Placeholder for the actual TV distance computation
# Compute the difference in regression coefficients
diff_beta0 <- (beta0_orig - mean(Y_prime - X_prime))^2
diff_beta1 <- (beta1_orig - mean(Y_prime / X_prime))^2
# Compute R^2 differences for each category in Z
R2_diff <- sum((R2_orig - p)^2)  # Placeholder for actual R^2 differences calculation
# Inter-cluster distance based on Z (as a placeholder)
inter_cluster_dist <- sum(abs(table(Z) - mean(Z)))  # Placeholder for actual inter-cluster calculation
# Combine all terms into the final loss function
loss <- lambda1 * (tvd_X + tvd_Y) + lambda2 * (diff_beta0 + diff_beta1) +
lambda3 * R2_diff + lambda4 * inter_cluster_dist
return(loss)
}
res = get_simpsons_paradox_c(x, y, z, t, sd_x = 0.07, sd_y = 0.07, lambda4 = 5)
# Generate data for two categories of z
# overall positive corr, works nice
set.seed(123)
n <- 300
z <- sample(c("A", "B", "C"), prob = c(0.3, 0.4, 0.3), size = n, replace = TRUE)
x <- rnorm(n, 10, sd = 5) + 5*rbeta(n, 5, 3)
y <- 2*x + rnorm(n, 5, sd = 4)
t = c(-0.8, 0.8, -0.8)
res = get_simpsons_paradox_c(x, y, z, t, sd_x = 0.07, sd_y = 0.07, lambda4 = 5)
library(devtools)
load_all(".")
# Generate data for two categories of z
# overall positive corr, works nice
set.seed(123)
n <- 300
z <- sample(c("A", "B", "C"), prob = c(0.3, 0.4, 0.3), size = n, replace = TRUE)
x <- rnorm(n, 10, sd = 5) + 5*rbeta(n, 5, 3)
y <- 2*x + rnorm(n, 5, sd = 4)
t = c(-0.8, 0.8, -0.8)
res = get_simpsons_paradox_c(x, y, z, t, sd_x = 0.07, sd_y = 0.07, lambda4 = 5)
library(devtools)
load_all(".")
library(tidyverse)
library(mvtnorm)
library(akima)
library(clue)
library(ggExtra)
library(gridExtra)
library(DescTools)
res = get_simpsons_paradox_c(x, y, z, t, sd_x = 0.07, sd_y = 0.07, lambda4 = 5)
library(devtools)
load_all(".")
library(tidyverse)
library(mvtnorm)
library(akima)
library(clue)
library(ggExtra)
library(gridExtra)
library(DescTools)
library(devtools)
load_all(".")
library(tidyverse)
library(mvtnorm)
library(akima)
library(clue)
library(ggExtra)
library(gridExtra)
library(DescTools)
res = get_simpsons_paradox_c(x, y, z, t, sd_x = 0.07, sd_y = 0.07, lambda4 = 5)
set.seed(42)
matrices <- list(
ta = matrix(c(512, 89, 313, 19), ncol = 2, byrow = TRUE),
tb = matrix(c(353, 17, 207, 8), ncol = 2, byrow = TRUE),
tc = matrix(c(120, 202, 205, 391), ncol = 2, byrow = TRUE),
td = matrix(c(138, 131, 279, 244), ncol = 2, byrow = TRUE),
te = matrix(c(53, 94, 138, 299), ncol = 2, byrow = TRUE),
tf = matrix(c(22, 24, 351, 317), ncol = 2, byrow = TRUE)
)
levels_z <- names(matrices)
df_list <- lapply(seq_along(matrices), function(i) {
mat <- matrices[[i]]
z_level <- levels_z[i]
df <- as.data.frame(as.table(mat))
colnames(df) <- c("x", "y", "Freq")
df$z <- z_level
return(df)
})
# Combine all the data frames into one
final_df <- do.call(rbind, df_list)
expanded_df <- final_df[rep(1:nrow(final_df), final_df$Freq), c("x", "y", "z")]
result <- get_simpsons_paradox_d(expanded_df$x, expanded_df$y, expanded_df$z,
manual_vec = c(-1, -1, -1, -1, -1, -1),
target_overall = +1,
margin = 0.2,  margin_overall = 0.2, max_n = 200)
table(expanded_df$x) - table(result$final_df$x)
table(expanded_df$y) - table(result$final_df$y)
table(expanded_df$z) - table(result$final_df$z)
?covalchemy::log_odds_dc
# Check and build
devtools::check()
# Check and build
devtools::check()
# Check and build
devtools::check()
library(devtools)
library(roxygen2)
load_all(".")
# Manual
devtools::document()
# Check and build
devtools::check()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
MASS::mvrnorm()
devtools::build()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
devtools::build()
?covalchemy
load_all(".")
?covalchemy
devtools::use_mit_license()
library(devtools)
library(roxygen2)
devtools::use_mit_license()
use_mit_license()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
# Check and build
devtools::check()
# Manual
devtools::document()
# Check and build
devtools::check()
library(interp)
install.packages("interp")
library(interp)
interp::aspline()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
devtools::build()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
# Check and build
devtools::check()
library(devtools)
library(roxygen2)
# Manual
devtools::document()
# Check and build
devtools::check()
devtools::build()
knitr::opts_chunk$set(message = FALSE, warning = FALSE,
include = TRUE,
fig.align = "center",  out.width = "80%")
library(tidyverse)
# FROM LAST TUTORIAL :)
tuesdata <- tidytuesdayR::tt_load(2021, week = 31)
olympics <- tuesdata$olympics
regions <- tuesdata$regions
df = olympics %>%
filter(season == "Summer") %>%
select(noc, year, event, medal)
summer_games = df %>%
group_by(year) %>%
summarise(n_nations = length(unique(noc)), # can also use n_distinct
n_events =  length(unique(event))) %>%
ungroup()
medals = df %>%
filter(year >= 1980) %>%
unique() %>%
group_by(noc, year) %>%
summarise(Gold = sum(medal == "Gold", na.rm = T),
Silver = sum(medal == "Silver", na.rm = T),
Bronze = sum(medal == "Bronze", na.rm = T),
Total = Gold + Silver + Bronze) %>%
ungroup() %>%
arrange(desc(Gold), desc(Silver), desc(Bronze))
# A plot on Number of nations and events over time
ggplot(data = summer_games) +
geom_line(aes(x = year, y = n_events), color = "indianred4", linewidth = 1.5) +
geom_line(aes(x = year, y = n_nations), color = "gray22", linewidth = 1.5) +
labs(title = "Number of Events and Nations in Summer Olympics", x = "", y = "") +
annotate("text", x = 2010, y = c(180, 280),
label = c("Nations", "Events"), color = c("gray22", "indianred4")) +
theme_minimal()
# Five countries that won the most golds in 2012
summer_top_5 <- medals %>%
filter(year == 2012) %>%
head(5) %>% pull(noc)
# Prepare data for gold medals received in each year from 2000 to 2012
df_plot <- medals  %>%
filter(noc %in% summer_top_5, year >= 2000, year <= 2012)
# Prepare the labels to use in the text geom
df_labels <- df_plot %>% filter(year == 2012) %>%
left_join(regions, by = c("noc" = "NOC"))
# Golds received across years
ggplot(df_plot, aes(x = year, y = Gold)) +
geom_line(aes(color = noc), size = 2, show.legend = FALSE) +
geom_point(aes(color = noc), size = 7, show.legend = FALSE) +
geom_label(data = df_labels, aes(label = region),
hjust = "left", nudge_x = 0.5, size = 3.5) +
labs(x = "", y = "Number of golds received", color = "Region",
title = "Gold medals received for selected countries", subtitle = "2000 - 2012") +
scale_x_continuous(breaks = seq(2000, 2012, 4), limits = c(2000, 2014)) +
theme_minimal()
# Compute the rankings in golds received in each year
ranks_top_5 <-  medals  %>%
group_by(year) %>%
mutate(rank = min_rank(-Gold)) %>%
filter(year >= 2000, year <= 2012, noc %in% summer_top_5)
ggplot(ranks_top_5, aes(x = year, y = rank, color = noc)) +
geom_point(size = 7, show.legend = F) +
geom_text(aes(label = rank)) +
geom_label(data = rank_labels, aes(label = region), hjust = "left", nudge_x = 0.5, size = 3.5, fill = "black") +
geom_line(size = 2, show.legend = F) +
labs(
title = "Rankings of golds received in 2000 and 2012",
subtitle = "... for countries that won the most golds in 2012",
x = "",
y = ""
) +
scale_x_continuous(breaks = seq(2000, 2012, 4), limits = c(2000, 2015)) +
scale_y_continuous(trans = "reverse", breaks = seq(1, 12)) +
theme_void()
ggplot(ranks_top_5, aes(x = year, y = rank, color = noc)) +
geom_point(size = 7, show.legend = F) +
geom_text(aes(label = rank)) +
geom_line(size = 2, show.legend = F) +
labs(
title = "Rankings of golds received in 2000 and 2012",
subtitle = "... for countries that won the most golds in 2012",
x = "",
y = ""
) +
scale_x_continuous(breaks = seq(2000, 2012, 4), limits = c(2000, 2015)) +
scale_y_continuous(trans = "reverse", breaks = seq(1, 12)) +
theme_void()
ranks_top_5
ggplot(ranks_top_5, aes(x = year, y = rank, color = noc)) +
geom_point(size = 7, show.legend = F) +
geom_line(size = 2, show.legend = F) +
labs(
title = "Rankings of golds received in 2000 and 2012",
subtitle = "... for countries that won the most golds in 2012",
x = "",
y = ""
) +
scale_x_continuous(breaks = seq(2000, 2012, 4), limits = c(2000, 2015)) +
scale_y_continuous(trans = "reverse", breaks = seq(1, 12)) +
theme_void() + geom_text(aes(label = rank))
ranks_top_5
ggplot(ranks_top_5, aes(x = year, y = rank, color = noc)) +
geom_point(size = 7, show.legend = F) +
geom_text(aes(label = rank))
ranks_top_5
ggplot(ranks_top_5, aes(x = year, y = rank, color = noc)) + geom_text(aes(label = rank))
ggplot(ranks_top_5, aes(x = year, y = rank, color = noc)) +
geom_point(size = 7, show.legend = F) +
geom_text(aes(label = rank), color = "black") +
geom_line(size = 2, show.legend = F) +
labs(
title = "Rankings of golds received in 2000 and 2012",
subtitle = "... for countries that won the most golds in 2012",
x = "",
y = ""
) +
scale_x_continuous(breaks = seq(2000, 2012, 4), limits = c(2000, 2015)) +
scale_y_continuous(trans = "reverse", breaks = seq(1, 12)) +
theme_void()
ggplot(ranks_top_5, aes(x = year, y = rank, color = noc)) +
geom_point(size = 7, show.legend = F) +
geom_text(aes(label = rank), color = "black") +
geom_line(size = 2, show.legend = F) +
labs(
title = "Rankings of golds received in 2000 and 2012",
subtitle = "... for countries that won the most golds in 2012",
x = "",
y = ""
) +
scale_x_continuous(breaks = seq(2000, 2012, 4), limits = c(2000, 2015)) +
scale_y_continuous(trans = "reverse", breaks = seq(1, 12)) +
theme_void()
df <- data.frame(x = sample(c("A", "B", "C"), 1000, replace = TRUE),
y = sample(c("D", "E", "F"), 1000, replace = TRUE))
target_entropy <- 1.5
result <- get_target_entropy(df$x, df$y, target_entropy)
final_df <- result$final_df
Use code with caution.
# Example 1: Modifying correlation
x <- rnorm(100)
y <- rnorm(100)
target_corr <- 0.5
res <- get_target_corr(x, y, target_corr)
modified_x <- res$x1
modified_y <- res$x2
